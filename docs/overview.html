<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>overview · Visionlab</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## T3LAB notes:"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="overview · Visionlab"/><meta property="og:type" content="website"/><meta property="og:url" content="https://visiont3lab.github.io/documentation/"/><meta property="og:description" content="## T3LAB notes:"/><meta property="og:image" content="https://visiont3lab.github.io/documentation/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://visiont3lab.github.io/documentation/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/documentation/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokai.min.css"/><link rel="alternate" type="application/atom+xml" href="https://visiont3lab.github.io/documentation/blog/atom.xml" title="Visionlab Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://visiont3lab.github.io/documentation/blog/feed.xml" title="Visionlab Blog RSS Feed"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/documentation/js/scrollSpy.js"></script><link rel="stylesheet" href="/documentation/css/main.css"/><script src="/documentation/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/documentation/"><img class="logo" src="/documentation/img/favicon.ico" alt="Visionlab"/><h2 class="headerTitleWithLogo">Visionlab</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/documentation/docs/deep_learning_setup_guide" target="_self">Docs</a></li><li class="siteNavGroupActive"><a href="/documentation/docs/docker_setup_guide" target="_self">API</a></li><li class=""><a href="/documentation/help" target="_self">Help</a></li><li class=""><a href="/documentation/blog/" target="_self">Blog</a></li><li class=""><a target="_self"></a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Research</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Installation Setup and useful scripts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/documentation/docs/deep_learning_setup_guide">Deep Learning Setup Guide</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/docker_setup_guide">Docker Setup Guide</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Research</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/documentation/docs/overview">overview</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/face_recognition">Face Recognition</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/object_detection">Object Detection</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/optical_character_recognition">Optical Character Recognition</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/photometry">Photometry</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/segmentation">Segmentation</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/streaming">Streaming</a></li><li class="navListItem"><a class="navItem" href="/documentation/docs/robotic_operating_system">Robotic Operating System</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Templates</h3><ul class=""><li class="navListItem"><a class="navItem" href="/documentation/docs/website_template">Website Template</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Notes</h3><ul class=""><li class="navListItem"><a class="navItem" href="/documentation/docs/common_issues">Common Issues</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/visiont3lab/documentation/edit/master/docs/overview.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">overview</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="t3lab-notes"></a><a href="#t3lab-notes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>T3LAB notes:</h2>
<ul>
<li><a href="https://site">xxxxx</a> [<em>comment</em>]</li>
<li><a href="https://site">xxxxx</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="distillation"></a><a href="#distillation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Distillation:</h2>
<ul>
<li><a href="https://github.com/dkozlov/awesome-knowledge-distillation">Disttilation GitHub</a> [<em>comment</em>]</li>
<li><a href="https://github.com/NervanaSystems/distiller">Disttilation Pytorch</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="kubernetes"></a><a href="#kubernetes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kubernetes:</h2>
<ul>
<li><a href="https://thenewstack.io/deploy-a-single-node-kubernetes-instance-in-seconds-with-microk8s/">MicroK8s</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="jetson"></a><a href="#jetson" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jetson:</h2>
<ul>
<li><a href="https://thenewstack.io/tutorial-configure-nvidia-jetson-nano-as-an-ai-testbed/">OpenCv Jetson+camera usb</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="parallel-thread-mpi4py"></a><a href="#parallel-thread-mpi4py" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Parallel Thread MPI4py:</h2>
<ul>
<li><a href="https://hpc-forge.cineca.it/files/ScuolaCalcoloParallelo_WebDAV/public/anno-2017/26th_Summer_School_on_Parallel_Computing/Bologna/">CINECA SUMMER PARALLEL MPI4py</a> [<em>comment</em>]</li>
<li><a href="https://www.packtpub.com/application-development/python-parallel-programming-solutions-video">Packt_parallel_comp</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="varie"></a><a href="#varie" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Varie:</h2>
<ul>
<li><a href="https://dev.to/graphtylove/how-to-automate-attendance-record-with-face-recognition-python-and-react-4413?fbclid=IwAR0UbGQ9p_SHhFyO6Dr3KHRVsqTfRMboPAOMqPWIjKPqO4Rs0hi2cQfrcsQ">REACT_CCTV</a> [<em>GOOD</em>]</li>
<li><a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace">streamlit</a> [<em>comment</em>]</li>
<li><a href="https://github.com/tugstugi/dl-colab-notebooks">COLAB DEEP LEARNING ADVANCED</a> [GOOD]</li>
<li><a href="https://github.com/tensorflow/tensorboard/tree/master/docs">Deep Learning in Medical AI 2018/2019</a> [<em>comment</em>]</li>
<li><a href="https://github.com/albu/albumentations">ALBUMENTATION</a> [LIBRARY COMPUTER_VISION_]</li>
<li><a href="https://github.com/JdeRobot/neuralFPGA">GOD-GITHUB</a> [<em>comment</em>]</li>
<li><a href="https://www.youtube.com/watch?v=KFrEPCOA0p4&amp;list=PLgB5c9xg9C91DJ30WFlHfHAhMECeho-gU&amp;index=2">GOD-YOUTUBE</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="describe-images"></a><a href="#describe-images" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DESCRIBE IMAGES:</h2>
<ul>
<li><a href="https://github.com/fawazsammani/look-and-modify/blob/master/README.md">PYTORCH DESCRIBE IMAGES</a> [<em>comment</em>]</li>
<li><a href="https://github.com/fawazsammani/knowing-when-to-look-adaptive-attention">PYTORCH</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="make-your-tiny-yolov3"></a><a href="#make-your-tiny-yolov3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MAKE YOUR TINY YOLOv3:</h2>
<ul>
<li><a href="https://www.ctolib.com/Deep-Learning-Studyroom-yolo_v3_tensorflow.html">tINY YOLO V3</a> [<em>YOLO CUSTOM</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="tvm"></a><a href="#tvm" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TVM</h2>
<ul>
<li><a href="https://sampl.cs.washington.edu/tvmconf/2018">TVM-VTA</a> [<em>comment</em>]</li>
<li><a href="https://www.groundai.com/project/vta-an-open-hardware-software-stack-for-deep-learning/1">VTA</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="pynq"></a><a href="#pynq" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pynq</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=nRxe-NqvOl8">cv2-pynq</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="xxx"></a><a href="#xxx" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>XXX</h2>
<ul>
<li><a href="https://site">xxxxx</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="dataset-from-oxford"></a><a href="#dataset-from-oxford" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dataset from Oxford</h2>
<ul>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/">best dataset image-video-text</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="mxnet"></a><a href="#mxnet" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MXNET</h2>
<ul>
<li><a href="https://www.d2l.ai/chapter_computer-vision/rcnn.html">School of MXNET-Gluon</a> [_good]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="opencv-deep-learning"></a><a href="#opencv-deep-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>OpenCV Deep Learning</h2>
<ul>
<li><a href="https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV">All models pre-trained for OpenCV</a> [_must to follow]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="pytorch"></a><a href="#pytorch" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>PYTORCH</h2>
<ul>
<li><a href="https://github.com/pawangeek/Pytorch-In-Real-Life">Pytorch-In-Real-Life</a> [_pYTORCH PROJETS]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="docker"></a><a href="#docker" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Docker</h2>
<ul>
<li><a href="https://www.codingforentrepreneurs.com/blog/jupyter-production-server-on-docker-heroku">jupyter-docker-Heroku</a> [_good]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="dataset"></a><a href="#dataset" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DATASET</h2>
<ul>
<li><a href="https://medium.com/@wengsengh/car-model-classification-e1ff09573f4f">CAR MODELS CLASSIFICATIONS</a> [<em>CAR MODELS</em>]</li>
<li><a href="http://yann.lecun.com/exdb/mnist/">MNIST Handwritten digits</a></li>
<li><a href="http://ufldl.stanford.edu/housenumbers/">Google House Numbers from street view</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 and CIFAR-100</a></li>
<li><a href="http://www.image-net.org/">IMAGENET</a></li>
<li><a href="http://groups.csail.mit.edu/vision/TinyImages/">Tiny Images 80 Million tiny images</a></li>
<li><a href="http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images">Flickr Data 100 Million Yahoo dataset</a></li>
<li><a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/">Berkeley Segmentation Dataset 500</a></li>
<li><a href="http://archive.ics.uci.edu/ml/">UC Irvine Machine Learning Repository</a></li>
<li><a href="http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html">Flickr 8k</a></li>
<li><a href="http://shannon.cs.illinois.edu/DenotationGraph/">Flickr 30k</a></li>
<li><a href="http://mscoco.org/home/">Microsoft COCO</a></li>
<li><a href="http://www.visualqa.org/">VQA</a></li>
<li><a href="http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/">Image QA</a></li>
<li><a href="http://www.uk.research.att.com/facedatabase.html">AT&amp;T Laboratories Cambridge face database</a></li>
<li><a href="http://xtreme.gsfc.nasa.gov">AVHRR Pathfinder</a></li>
<li><a href="http://www.anc.ed.ac.uk/~amos/afreightdata.html">Air Freight</a> - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)</li>
<li><a href="http://www.science.uva.nl/~aloi/">Amsterdam Library of Object Images</a> - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)</li>
<li><a href="http://www.imm.dtu.dk/~aam/">Annotated face, hand, cardiac &amp; meat images</a> - Most images &amp; annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)</li>
<li><a href="http://www.imm.dtu.dk/image/">Image Analysis and Computer Graphics</a></li>
<li><a href="http://www.cog.brown.edu/~tarr/stimuli.html">Brown University Stimuli</a> - A variety of datasets including geons, objects, and &quot;greebles&quot;. Good for testing recognition algorithms. (Formats: pict)</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/">CAVIAR video sequences of mall and public space behavior</a> - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 &amp; JPEG)</li>
<li><a href="http://www.ipab.inf.ed.ac.uk/mvu/">Machine Vision Unit</a></li>
<li><a href="http://www.cs.waikato.ac.nz/~singlis/ccitt.html">CCITT Fax standard images</a> - 8 images (Formats: gif)</li>
<li><a href="cil-ster.html">CMU CIL's Stereo Data with Ground Truth</a> - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)</li>
<li><a href="http://www.ri.cmu.edu/projects/project_418.html">CMU PIE Database</a> - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.</li>
<li><a href="http://www.ius.cs.cmu.edu/idb/">CMU VASC Image Database</a> - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)</li>
<li><a href="http://www.vision.caltech.edu/html-files/archive.html">Caltech Image Database</a> - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)</li>
<li><a href="http://www.cs.columbia.edu/CAVE/curet/">Columbia-Utrecht Reflectance and Texture Database</a> - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)</li>
<li><a href="http://www.cs.sfu.ca/~colour/data/index.html">Computational Colour Constancy Data</a> - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, camera sensor data, and over 700 images. (Formats: tiff)</li>
<li><a href="http://www.cs.sfu.ca/~colour/">Computational Vision Lab</a></li>
<li><a href="http://www.cs.washington.edu/research/imagedatabase/groundtruth/">Content-based image retrieval database</a> - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)</li>
<li><a href="http://www.cs.washington.edu/research/imagedatabase/">Efficient Content-based Retrieval Group</a></li>
<li><a href="http://ls7-www.cs.uni-dortmund.de/~peters/pages/research/modeladaptsys/modeladaptsys_vba_rov.html">Densely Sampled View Spheres</a> - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)</li>
<li><a href="http://ls7-www.cs.uni-dortmund.de/">Computer Science VII (Graphical Systems)</a></li>
<li><a href="http://vision.psych.umn.edu/www/kersten-lab/demos/digitalembryo.html">Digital Embryos</a> - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)</li>
<li><a href="http://vision.psych.umn.edu/www/kersten-lab/kersten-lab.html">Univerity of Minnesota Vision Lab</a></li>
<li><a href="http://www.gastrointestinalatlas.com">El Salvador Atlas of Gastrointestinal VideoEndoscopy</a> - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)</li>
<li><a href="http://sting.cycollege.ac.cy/~alanitis/fgnetaging/index.htm">FG-NET Facial Aging Database</a> - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)</li>
<li><a href="http://bias.csr.unibo.it/fvc2000/">FVC2000 Fingerprint Databases</a> - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).</li>
<li><a href="http://bias.csr.unibo.it/research/biolab">Biometric Systems Lab</a> - University of Bologna</li>
<li><a href="http://www.fg-net.org">Face and Gesture images and image sequences</a> - Several image datasets of faces and gestures that are ground truth annotated for benchmarking</li>
<li><a href="http://www-i6.informatik.rwth-aachen.de/~dreuw/database.html">German Fingerspelling Database</a> - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)</li>
<li><a href="http://www-i6.informatik.rwth-aachen.de/">Language Processing and Pattern Recognition</a></li>
<li><a href="http://hlab.phys.rug.nl/archive.html">Groningen Natural Image Database</a> - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)</li>
<li><a href="http://www.icg.tu-graz.ac.at/~schindler/Data">ICG Testhouse sequence</a> -  2 turntable sequences from ifferent viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)</li>
<li><a href="http://www.icg.tu-graz.ac.at">Institute of Computer Graphics and Vision</a></li>
<li><a href="http://www.ien.it/is/vislib/">IEN Image Library</a> - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)</li>
<li><a href="http://www-rocq.inria.fr/~tarel/syntim/images.html">INRIA's Syntim images database</a> - 15 color image of simple objects (Formats: gif)</li>
<li><a href="http://www.inria.fr/">INRIA</a></li>
<li><a href="http://www-rocq.inria.fr/~tarel/syntim/paires.html">INRIA's Syntim stereo databases</a> - 34 calibrated color stereo pairs (Formats: gif)</li>
<li><a href="http://www.ece.ncsu.edu/imaging/Archives/ImageDataBase/index.html">Image Analysis Laboratory</a> - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of &quot;medical images&quot;. (Formats: homebrew)</li>
<li><a href="http://www.ece.ncsu.edu/imaging">Image Analysis Laboratory</a></li>
<li><a href="http://www.prip.tuwien.ac.at/prip/image.html">Image Database</a> - An image database including some textures</li>
<li><a href="http://www.mis.atr.co.jp/~mlyons/jaffe.html">JAFFE Facial Expression Image Database</a> - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)</li>
<li><a href="http://www.mic.atr.co.jp/">ATR Research, Kyoto, Japan</a></li>
<li><a href="ftp://ftp.vislist.com/IMAGERY/JISCT/">JISCT Stereo Evaluation</a> - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)</li>
<li><a href="http://www-white.media.mit.edu/vismod/imagery/VisionTexture/vistex.html">MIT Vision Texture</a> - Image archive (100+ images) (Formats: ppm)</li>
<li><a href="ftp://whitechapel.media.mit.edu/pub/images">MIT face images and more</a> - hundreds of images (Formats: homebrew)</li>
<li><a href="http://vision.cse.psu.edu/book/testbed/images/">Machine Vision</a> - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)</li>
<li><a href="http://marathon.csee.usf.edu/Mammography/Database.html">Mammography Image Databases</a> - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)</li>
<li><a href="ftp://ftp.cps.msu.edu/pub/prip">ftp://ftp.cps.msu.edu/pub/prip</a> - many images (Formats: unknown)</li>
<li><a href="http://www.middlebury.edu/stereo/data.html">Middlebury Stereo Data Sets with Ground Truth</a> - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)</li>
<li><a href="http://www.middlebury.edu/stereo">Middlebury Stereo Vision Research Page</a> - Middlebury College</li>
<li><a href="http://ltpwww.gsfc.nasa.gov/MODIS/MAS/">Modis Airborne simulator, Gallery and data set</a> - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)</li>
<li><a href="ftp://sequoyah.ncsl.nist.gov/pub/databases/data">NIST Fingerprint and handwriting</a> - datasets - thousands of images (Formats: unknown)</li>
<li><a href="ftp://ftp.cs.columbia.edu/jpeg/other/uuencoded">NIST Fingerprint data</a> - compressed multipart uuencoded tar file</li>
<li><a href="http://www.nlm.nih.gov/research/visible/visible_human.html">NLM HyperDoc Visible Human Project</a> - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)</li>
<li><a href="http://www.designrepository.org">National Design Repository</a> - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineerign designs. (Formats: gif,vrml,wrl,stp,sat)</li>
<li><a href="http://gicl.mcs.drexel.edu">Geometric &amp; Intelligent Computing Laboratory</a></li>
<li><a href="http://eewww.eng.ohio-state.edu/~flynn/3DDB/Models/">OSU (MSU) 3D Object Model Database</a> - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)</li>
<li><a href="http://eewww.eng.ohio-state.edu/~flynn/3DDB/RID/">OSU (MSU/WSU) Range Image Database</a> - Hundreds of real and synthetic images (Formats: gif, homebrew)</li>
<li><a href="http://sampl.eng.ohio-state.edu/~sampl/database.htm">OSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences</a> - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)</li>
<li><a href="http://sampl.eng.ohio-state.edu">Signal Analysis and Machine Perception Laboratory</a></li>
<li><a href="http://www.cs.otago.ac.nz/research/vision/Research/OpticalFlow/opticalflow.html">Otago Optical Flow Evaluation Sequences</a> - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)</li>
<li><a href="http://www.cs.otago.ac.nz/research/vision/index.html">Vision Research Group</a></li>
<li><a href="ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/">ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/</a> - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))</li>
<li><a href="http://www.limsi.fr/Recherche/IMM/PageIMM.html">LIMSI-CNRS/CHM/IMM/vision</a></li>
<li><a href="http://www.limsi.fr/">LIMSI-CNRS</a></li>
<li><a href="http://www.taurusstudio.net/research/pmtexdb/index.htm">Photometric 3D Surface Texture Database</a> - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)</li>
<li><a href="http://www.cee.hw.ac.uk/~mtc/sofa">SEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA)</a> - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)</li>
<li><a href="http://www.cee.hw.ac.uk/~mtc/research.html">Computer Vision Group</a></li>
<li><a href="http://www.nada.kth.se/~zucch/CAMERA/PUB/seq.html">Sequences for Flow Based Reconstruction</a> - synthetic sequence for testing structure from motion algorithms (Formats: pgm)</li>
<li><a href="http://www-dbv.cs.uni-bonn.de/stereo_data/">Stereo Images with Ground Truth Disparity and Occlusion</a> - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)</li>
<li><a href="http://range.informatik.uni-stuttgart.de">Stuttgart Range Image Database</a> - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)</li>
<li><a href="http://www.informatik.uni-stuttgart.de/ipvr/bv/bv_home_engl.html">Department Image Understanding</a></li>
<li><a href="http://rvl1.ecn.purdue.edu/~aleix/aleix_face_DB.html">The AR Face Database</a> - Contains over 4,000 color images corresponding to 126 people's faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))</li>
<li><a href="http://rvl.www.ecn.purdue.edu/RVL/">Purdue Robot Vision Lab</a></li>
<li><a href="http://web.mit.edu/torralba/www/database.html">The MIT-CSAIL Database of Objects and Scenes</a> - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)</li>
<li><a href="http://rvl1.ecn.purdue.edu/RVL/specularity_database/">The RVL SPEC-DB (SPECularity DataBase)</a> - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )</li>
<li><a href="http://rvl1.ecn.purdue.edu/RVL/">Robot Vision Laboratory</a></li>
<li><a href="http://xm2vtsdb.ee.surrey.ac.uk">The Xm2vts database</a> - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.</li>
<li><a href="http://www.ee.surrey.ac.uk/Research/CVSSP">Centre for Vision, Speech and Signal Processing</a></li>
<li><a href="http://i21www.ira.uka.de/image_sequences">Traffic Image Sequences and 'Marbled Block' Sequence</a> - thousands of frames of digitized traffic image sequences as well as the 'Marbled Block' sequence (grayscale images) (Formats: GIF)</li>
<li><a href="http://i21www.ira.uka.de">IAKS/KOGS</a></li>
<li><a href="ftp://ftp.iam.unibe.ch/pub/Images/FaceImages">U Bern Face images</a> - hundreds of images (Formats: Sun rasterfile)</li>
<li><a href="ftp://freebie.engin.umich.edu/pub/misc/textures">U Michigan textures</a> (Formats: compressed raw)</li>
<li><a href="http://www.ee.oulu.fi/~olli/Projects/Lumber.Grading.html">U Oulu wood and knots database</a> - Includes classifications - 1000+ color images (Formats: ppm)</li>
<li><a href="http://vision.doc.ntu.ac.uk/datasets/UCID/ucid.html">UCID - an Uncompressed Colour Image Database</a> - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)</li>
<li><a href="http://vis-www.cs.umass.edu/vislib/directory.html">UMass Vision Image Archive</a> - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)</li>
<li><a href="ftp://sunsite.unc.edu/pub/academic/computer-science/virtual-reality/3d">UNC's 3D image database</a> - many images (Formats: GIF)</li>
<li><a href="http://marathon.csee.usf.edu/range/seg-comp/SegComp.html">USF Range Image Data with Segmentation Ground Truth</a> - 80 image sets (Formats: Sun rasterimage)</li>
<li><a href="http://www.ee.oulu.fi/research/imag/color/pbfd.html">University of Oulu Physics-based Face Database</a> - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.</li>
<li><a href="http://www.ee.oulu.fi/mvmp/">Machine Vision and Media Processing Unit</a></li>
<li><a href="http://www.outex.oulu.fi">University of Oulu Texture Database</a> - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)</li>
<li><a href="http://www.ee.oulu.fi/mvg">Machine Vision Group</a></li>
<li><a href="ftp://ftp.uu.net/published/usenix/faces">Usenix face database</a> - Thousands of face images from many different sites (circa 994)</li>
<li><a href="http://www-prima.inrialpes.fr/Prima/hall/view_sphere.html">View Sphere Database</a> - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)</li>
<li><a href="http://www-prima.inrialpes.fr/Prima/">PRIMA, GRAVIR</a></li>
<li><a href="ftp://ftp.vislist.com/IMAGERY/">Vision-list Imagery Archive</a> - Many images, many formats</li>
<li><a href="http://www.cs.cmu.edu/~owenc/word.htm">Wiry Object Recognition Database</a> - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)</li>
<li><a href="http://www.cs.cmu.edu/0.000000E+003dvision/">3D Vision Group</a></li>
<li><a href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html">Yale Face Database</a> -  165 images (15 individuals) with different lighting, expression, and occlusion configurations.</li>
<li><a href="http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html">Yale Face Database B</a> - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)</li>
<li><a href="http://cvc.yale.edu/">Center for Computational Vision and Control</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="r-cnn"></a><a href="#r-cnn" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>R-CNN</h2>
<ul>
<li><a href="https://github.com/VisualComputingInstitute/TrackR-CNN/tree/master">VisualComputingInstitute/TrackR-CNN</a> [<em>comment</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="convert-pytorch-models-in-production"></a><a href="#convert-pytorch-models-in-production" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Convert PyTorch Models in Production:</h2>
<ul>
<li><a href="https://github.com/MTlab/onnx2caffe">Onnx-2-caffe</a> [<em>good</em>]</li>
<li><a href="https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html#sphx-glr-download-advanced-super-resolution-with-caffe2-py">TRANSFERING A MODEL FROM PYTORCH TO CAFFE2 AND MOBILE USING ONNX</a> [_GOOD]</li>
<li><a href="https://pytorch.org/tutorials/#production-usage">PyTorch Production Level Tutorials</a> [<em>Fantastic</em>]</li>
<li><a href="https://pytorch.org/2018/05/02/road-to-1.0.html">The road to 1.0: production ready PyTorch</a></li>
<li><a href="http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/">PyTorch 1.0 tracing JIT and LibTorch C++ API to integrate PyTorch into NodeJS</a> [<em>Good Article</em>]</li>
<li><a href="https://pytorch.org/blog/model-serving-in-pyorch/">Model Serving in PyTorch</a></li>
<li><a href="https://pytorch.devpost.com/">PyTorch Summer Hackathon</a> [<em>Very Important</em>]</li>
<li><a href="https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html">Deploying PyTorch and Building a REST API using Flask</a> [<em>Important</em>]</li>
<li><a href="https://github.com/jaroslaw-weber/hotdog-not-hotdog">PyTorch model recognizing hotdogs and not-hotdogs deployed on flask</a></li>
<li><a href="https://github.com/Wizaron/pytorch-cpp-inference">Serving PyTorch 1.0 Models as a Web Server in C++ </a> [<em>Useful Example</em>]</li>
<li><a href="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch Internals</a>  [<em>Interesting &amp; Useful Article</em>]</li>
<li><a href="https://github.com/craigsidcarlson/PytorchFlaskApp">Flask application to support pytorch model prediction</a></li>
<li><a href="https://discuss.pytorch.org/t/serving-pytorch-model-on-flask-thread-safety/13921">Serving PyTorch Model on Flask Thread-Safety</a></li>
<li><a href="https://machinelearnings.co/serving-pytorch-models-on-aws-lambda-with-caffe2-onnx-7b096806cfac">Serving PyTorch Models on AWS Lambda with Caffe2 &amp; ONNX</a></li>
<li><a href="https://blog.waya.ai/deploy-deep-machine-learning-in-production-the-pythonic-way-a17105f1540e">Serving PyTorch Models on AWS Lambda with Caffe2 &amp; ONNX (Another Version)</a></li>
<li><a href="https://euclidesdb.readthedocs.io/en/latest/">EuclidesDB - <em>multi-model machine learning feature database with PyTorch</em></a></li>
<li><a href="https://github.com/perone/euclidesdb/">EuclidesDB - GitHub</a></li>
<li><a href="https://github.com/mil-tokyo/webdnn">WebDNN: Fastest DNN Execution Framework on Web Browser</a></li>
<li><a href="https://github.com/alecrubin/pytorch-serverless/">FastAI PyTorch Serverless API (with AWS Lambda)</a></li>
<li><a href="http://forums.fast.ai/t/fastai-pytorch-in-production/16928">FastAI PyTorch in Production (discussion)</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="funny-projects"></a><a href="#funny-projects" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Funny Projects:</h2>
<ul>
<li><a href="https://github.com/vietnguyen1991/AirGesture">control videogame with hand-code</a> [_]</li>
<li><a href="https://www.youtube.com/watch?v=TT2Tjd_h9ew">control videogame with hand-video</a> [_]</li>
<li><a href="https://youtu.be/dCKbRCUyop8">Learn how to morph faces with a Generative Adversarial Network!</a> [_GAN ]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="start-with-onnx-projects"></a><a href="#start-with-onnx-projects" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>START with ONNX Projects:</h2>
<ul>
<li><a href="https://leimao.github.io/blog/PyTorch-ATen-ONNX/">ONNX problem ATen</a></li>
<li><a href="https://github.com/mufajjul/onnx-operationalisation">START WITH ONNX</a></li>
<li><a href="https://cloudblogs.microsoft.com/opensource/2019/05/22/onnx-runtime-machine-learning-inferencing-0-4-release/">ONNX RUNTIME</a></li>
<li><a href="https://github.com/microsoft/onnxruntime">ONNX RUNTIME-NOTEBOOK AND TUTORIAL</a></li>
<li><a href="https://cloudblogs.microsoft.com/opensource/2019/08/26/announcing-onnx-runtime-0-5-edge-hardware-acceleration-support/">ONNX RUNTIME-OPENVINO</a></li>
<li><a href="https://channel9.msdn.com/Shows/Internet-of-Things-Show/Train-with-Azure-ML-and-deploy-everywhere-with-ONNX-Runtime">ONNX RUNTIME- complete tutorial JETSON NANO+ UP SQUARED</a> [FANTASTIC]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="keras-model-from-google"></a><a href="#keras-model-from-google" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Keras Model from Google:</h2>
<ul>
<li><a href="https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo">python models for deep learning</a> [<em>to must to see</em>]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="acellerate-with-fpga"></a><a href="#acellerate-with-fpga" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acellerate with FPGA:</h2>
<ul>
<li><a href="http://www.innovatefpga.com/cgi-bin/innovate/teams.pl?Id=EM031">FPGA-Python custom -Verilog</a> [FPGA]</li>
<li><a href="https://www.nextplatform.com/2018/07/24/clearing-the-tensorflow-to-fpga-path/">LE-Flow- BLOG</a> [FPGA_]</li>
<li><a href="https://github.com/danielholanda/LeFlow/tree/master/examples">LE-Flow TF to FPGA</a> [_FPGA]</li>
<li><a href="https://www.youtube.com/watch?v=b_KXFaFRGRY&amp;t=1326s">Accelerating Python with FPGAs</a> [ FPGA ]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="project-zoo"></a><a href="#project-zoo" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Project Zoo:</h2>
<ul>
<li><a href="https://modelzoo.co/">Project Zoo</a> [_BEST EVER]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="ocr-tesseract"></a><a href="#ocr-tesseract" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>OCR - Tesseract</h2>
<ul>
<li><a href="https://github.com/tesseract-ocr/tesseract">Tesseract Open Source OCR Engine (main repository)</a> [_from google]</li>
<li><a href="https://github.com/astutejoe/tesseractfonts">Tesseract import new fonts</a> [from developer]</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="xxx-1"></a><a href="#xxx-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>XXX</h2>
<ul>
<li><a href="https://site">xxxxx</a> [<em>comment</em>]</li>
<li><a href="https://site">xxxxx</a> [<em>comment</em>]</li>
</ul>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/documentation/docs/docker_setup_guide"><span class="arrow-prev">← </span><span>Docker Setup Guide</span></a><a class="docs-next button" href="/documentation/docs/face_recognition"><span>Face Recognition</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#t3lab-notes">T3LAB notes:</a></li><li><a href="#distillation">Distillation:</a></li><li><a href="#kubernetes">Kubernetes:</a></li><li><a href="#jetson">Jetson:</a></li><li><a href="#parallel-thread-mpi4py">Parallel Thread MPI4py:</a></li><li><a href="#varie">Varie:</a></li><li><a href="#describe-images">DESCRIBE IMAGES:</a></li><li><a href="#make-your-tiny-yolov3">MAKE YOUR TINY YOLOv3:</a></li><li><a href="#tvm">TVM</a></li><li><a href="#pynq">Pynq</a></li><li><a href="#xxx">XXX</a></li><li><a href="#dataset-from-oxford">Dataset from Oxford</a></li><li><a href="#mxnet">MXNET</a></li><li><a href="#opencv-deep-learning">OpenCV Deep Learning</a></li><li><a href="#pytorch">PYTORCH</a></li><li><a href="#docker">Docker</a></li><li><a href="#dataset">DATASET</a></li><li><a href="#r-cnn">R-CNN</a></li><li><a href="#convert-pytorch-models-in-production">Convert PyTorch Models in Production:</a></li><li><a href="#funny-projects">Funny Projects:</a></li><li><a href="#start-with-onnx-projects">START with ONNX Projects:</a></li><li><a href="#keras-model-from-google">Keras Model from Google:</a></li><li><a href="#acellerate-with-fpga">Acellerate with FPGA:</a></li><li><a href="#project-zoo">Project Zoo:</a></li><li><a href="#ocr-tesseract">OCR - Tesseract</a></li><li><a href="#xxx-1">XXX</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/documentation/" class="nav-home"><img src="/documentation/img/favicon.ico" alt="Visionlab" width="66" height="58"/></a><div><h5>Docs</h5><a href="/documentation/docs/en/doc1.html">Getting Started (or other categories)</a><a href="/documentation/docs/en/doc2.html">Guides (or other categories)</a><a href="/documentation/docs/en/doc3.html">API Reference (or other categories)</a></div><div><h5>Community</h5><a href="/documentation/en/users.html">User Showcase</a><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://discordapp.com/">Project Chat</a><a href="https://twitter.com/" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="/documentation/blog">Blog</a><a href="https://github.com/">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/documentation/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2019 Your Name or Your Company Name</section></footer></div></body></html>